#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de diagnostic pour la quantization
Teste si bitsandbytes fonctionne correctement avec votre GPU
"""

import sys
import torch

print("="*60)
print("üîç DIAGNOSTIC QUANTIZATION")
print("="*60)

# 1. Test PyTorch et CUDA
print("\n1Ô∏è‚É£ PyTorch et CUDA:")
print(f"   PyTorch version: {torch.__version__}")
print(f"   CUDA disponible: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"   CUDA version: {torch.version.cuda}")
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB")
else:
    print("   ‚ùå CUDA non disponible!")
    sys.exit(1)

# 2. Test bitsandbytes
print("\n2Ô∏è‚É£ Bitsandbytes:")
try:
    import bitsandbytes as bnb
    print(f"   ‚úÖ bitsandbytes version: {bnb.__version__}")

    # Test CUDA ops
    try:
        # Test si les op√©rations CUDA sont disponibles
        test_tensor = torch.randn(10, 10).cuda()
        print(f"   ‚úÖ CUDA ops disponibles")
    except Exception as e:
        print(f"   ‚ö†Ô∏è CUDA ops error: {e}")

except ImportError as e:
    print(f"   ‚ùå bitsandbytes non install√©: {e}")
    print(f"   üí° Installez avec: pip install bitsandbytes")
    sys.exit(1)
except Exception as e:
    print(f"   ‚ùå Erreur: {e}")
    sys.exit(1)

# 3. Test transformers et BitsAndBytesConfig
print("\n3Ô∏è‚É£ Transformers:")
try:
    import transformers
    from transformers import BitsAndBytesConfig
    print(f"   ‚úÖ transformers version: {transformers.__version__}")
    print(f"   ‚úÖ BitsAndBytesConfig disponible")
except ImportError as e:
    print(f"   ‚ùå Erreur import: {e}")
    sys.exit(1)

# 4. Test de chargement avec quantization
print("\n4Ô∏è‚É£ Test de chargement quantiz√©:")
print("   Tentative de chargement d'un petit mod√®le en int8...")

try:
    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

    model_name = "facebook/nllb-200-distilled-600M"

    # Test INT8
    print(f"   üì• Chargement {model_name} en INT8...")

    quantization_config = BitsAndBytesConfig(
        load_in_8bit=True,
        llm_int8_threshold=6.0,
        llm_int8_has_fp16_weight=False
    )

    model = AutoModelForSeq2SeqLM.from_pretrained(
        model_name,
        quantization_config=quantization_config,
        device_map="auto",
        torch_dtype=torch.bfloat16
    )

    print(f"   ‚úÖ Chargement INT8 r√©ussi!")

    # V√©rifier la VRAM utilis√©e
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / (1024**3)
        reserved = torch.cuda.memory_reserved() / (1024**3)
        print(f"   üìä VRAM allou√©e: {allocated:.2f} GB")
        print(f"   üìä VRAM r√©serv√©e: {reserved:.2f} GB")

    # Nettoyage
    del model
    torch.cuda.empty_cache()

    print("\n" + "="*60)
    print("‚úÖ TOUS LES TESTS R√âUSSIS!")
    print("="*60)
    print("\nüí° La quantization devrait fonctionner correctement.")
    print("   Si vous avez toujours des probl√®mes, partagez les logs.")

except Exception as e:
    print(f"   ‚ùå Erreur lors du chargement: {e}")
    print(f"\nüìã Type d'erreur: {type(e).__name__}")
    import traceback
    print("\nüìã Traceback complet:")
    traceback.print_exc()

    print("\n" + "="*60)
    print("‚ùå √âCHEC DU TEST")
    print("="*60)

    # Suggestions de solutions
    print("\nüí° Solutions possibles:")
    print("   1. R√©installez bitsandbytes:")
    print("      pip uninstall bitsandbytes -y")
    print("      pip install bitsandbytes>=0.41.0")
    print()
    print("   2. V√©rifiez la compatibilit√© CUDA:")
    print("      pip install torch --upgrade --index-url https://download.pytorch.org/whl/cu121")
    print()
    print("   3. Si vous utilisez WSL2, installez CUDA Toolkit dans WSL")
    print()
    print("   4. Essayez sans quantization (none) pour v√©rifier que le reste fonctionne")

    sys.exit(1)
